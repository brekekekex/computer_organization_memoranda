{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M06. Little's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definitions of the terms *response time* and *throughput*. Both are measures of a technology's performance with respect to time, but they are not equivalent measures. Consider an arbitrary queuing system (e.g., the University of Rochester, the McDonald's on Mt. Hope Ave. in College Town, the Toyota Motor Corp.'s Takaoka Plant Number 1) that services, or *operates on*, a particular task (respectively, the higher education of young minds, the taking of fast-food orders, and the mass-production of Corollas). More formally, within computer engineering, we often say that a queuing system is endowed with a stream of discrete *requests*, which, having arrived at some rate into the system, join one of perhaps multiple queues, are eventually serviced, and then depart.\n",
    "\n",
    "By *response time*, we refer to the elapsed time between a request's arrival and the system effecting its response to said request (resp., four to five years to graduate a typical freshman, however long it takes to advance a customer to the register, and about 60 seconds for a completed automobile to roll off the assembly line). While the literature seems to endlessly discriminate between a system's response time and its *latency*, in practice, latency is the average of response times over some operational interval. From the perspective of any one item, the latency of a system is its expected *sojourn*, or waiting, time.\n",
    "\n",
    "By *throughput*, we refer to the time-rate at which a system processes requests (resp., roughly a thousand students per year, maybe two customers every five minutes, and around 250,000 cars per year). Since a request is considered to undergo processing upon arrival, one often sees throughput given as the average *arrival rate* of requests *into* the system. \n",
    "\n",
    "It takes hardly any dimensional analysis to observe that latency is given in time while throughput is measured in requests per unit time. Clearly, the two are *not* alike. A different (and perhaps less redundant) relationship is instead suggested: *could latency and throughput be inversely proportional?*\n",
    "\n",
    "Little's Law (John D. C. Little, 1961) is a classical result from *queuing theory* that answers this question in the affirmative&mdash;under certain conditions. It is stated as follows: \n",
    "for a queuing system with steady-state average queue-size $L$, steady-state throughput $\\lambda$, and steady-state latency $W$, the following formula holds\n",
    "\n",
    "$$L = \\lambda W,$$\n",
    "\n",
    "of which we will supply a toy derivation.\n",
    "\n",
    "Consider a system $\\Sigma$ that is empty of tasks at time $t=0$, and observe its queuing process over the time-interval $[0,T]$. Denote the following: \n",
    "\n",
    "- $n(t)$, the number of requests in $\\Sigma$ at time $t$.\n",
    "- $\\lambda$, the average arrival rate of requests in $[0,T]$ (a.k.a. throughput, given in requests per unit time).\n",
    "- $N$, the number of requests arrived during $[0,T]$.\n",
    "- $L$, the average number of requests in $\\Sigma$ during $[0,T]$ (a.k.a. average queue-size, given in requests).\n",
    "- $W$, the average waiting time of a request during $[0,T]$ (a.k.a. latency, given in time).\n",
    "\n",
    "Begin by defining $A = \\int_{0}^{T}n(t)dt$, which gives the area under $n(t)$ in units of request-time. Then, we have \n",
    "$$L = \\frac{A}{T}$$\n",
    "$$\\lambda = \\frac{N}{T}$$\n",
    "$$W = \\frac{A}{N}.$$\n",
    "\n",
    "Of these equations, the third deserves some further elaboration. To wit, $A$ is effectively a measure of total *work* accomplished by $\\Sigma$, which, averaged over $N$, the total number of items tasked to $\\Sigma$, yields latency.\n",
    "\n",
    "The desired result is then readily obtained\n",
    "$$L = \\frac{A}{T} = \\frac{N}{T}\\frac{A}{N} = \\lambda W.$$\n",
    "\n",
    "The immediate utility of this formula is that given any two of the three parameters $N$, $\\lambda$, and $W$, it is possible to compute the third. For example, the University of Rochester's undergraduate program has a steady-state latency of approximately 4.5 years and a steady-state throughput of approximately 1400 students per year. We might therefore expect a queue-size (an undergraduate population) of $4.5$ years multiplied by $1400$ students/year, which comes out to roughly $6300$ total undergraduates enrolled at any one time. \n",
    "\n",
    "In a (fictional) country with a steady-state population, one imagines that *life itself* could be seen as a queuing process, with the lifetime of citizens merely being their latency from birth to death.\n",
    "\n",
    "As for the consequence of original interest, for a system with a steady-state queue-size, throughput is indeed inversely proportional to latency and vice-versa (this is *not* the same as reciprocality):\n",
    "$$\\lambda \\propto \\frac{c}{W}$$\n",
    "$$W \\propto \\frac{c}{\\lambda},$$\n",
    "\n",
    "from which one obtains the limiting behaviors\n",
    "$$\\lim_{W\\to \\infty} \\lambda = 0$$\n",
    "$$\\lim_{\\lambda \\to 0} W = 0.$$\n",
    "\n",
    "# Annotated Bibliography\n",
    "\n",
    "- The proof presented with commentary above is the first given in John D. C. Little's *Little's Law as Viewed on its 50th Anniversary*. For convenience, I have modified some terminology (with respect to queuing theory as it is used within computer engineering)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
